

DART - trees added early are significant and trees added late are unimportant.

Features
- solve over-fitting - 
- can be solve
- early stop might not be stable

dart inherits gbtree

- eta
- gamm 
- max_depth

- sample type: sampling algorithm
 uniform: dropped trees are selected uniformly
 weighted: dropped trees are selected in proportion to weight

- normalize type: type of normalization algorithm
 tree: new trees hate the sane weight of each of dropped trees
 forest: nee trees have the same weight of sum of dropped trees

-rate_drop: 0-1

-skip_drop: probability of skipping dropout
	if a dropout is skipped, new trees are added in the same manner as gbtree
	range 0-1

_______
ntree_limit - specify when predictioon with test sets. By default, bst.predict() will perform dropouts on trees. To obtain correct results on test sets, disable dropouts by specifying a nonzero value for ntree_limit
______________________
For tree models, it is important to use consistent data formats during training and scoring.
______________________
Note that the gblinear booster treats missing values as zeros.
_______________________
Suppose the following code fits your model without monotonicity constraints

model_no_constraints = xgb.train(params, dtrain,
                               num_boost_round = 1000, evals = evallist,
                                 early_stopping_rounds = 10)

Then fitting with monotonicity constraints only requires adding a single parameter

params_constrained = params.copy()
params_constrained['monotone_constraints'] = "(1,-1)"

model_with_constraints = xgb.train(params_constrained, dtrain,
                                   num_boost_round = 1000, evals = evallist,
                                   early_stopping_rounds = 10)


In this example the training data X has two columns, and by using the parameter values (1,-1) we are telling XGBoost to impose an increasing constraint on the first predictor and a decreasing constraint on the second.

Choice of tree construction algorithm. To use monotonic constraints, be sure to set the tree_method parameter to one of exact, hist, and gpu_hist.

Note for the ‘hist’ tree construction algorithm.
If tree_method is set to either hist or gpu_hist, enabling monotonic constraints may produce unnecessarily shallow trees. This is because the hist method reduces the number of candidate splits to be considered at each split. Monotonic constraints may wipe out all available split candidates, in which case no split is made. To reduce the effect, you may want to increase the max_bin parameter to consider more split candidates.
______________________________
Feature interaction constraints
+Better predictive performance from focusing on interactions that work – whether through domain specific knowledge or algorithms that rank interactions
+Less noise in predictions; better generalization
+More control to the user on what the model can fit. For example, the user may want to exclude some interactions even if they perform well due to regulatory constraints