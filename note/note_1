# ___________________________Дерево решение - практика
# 1 . генерирум даннные

# первый класс
np.seed = 7
train_data = np.random.normal(size=(100, 2))
train_labels = np.zeros(100)

# добавляем второй класс
train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]
train_labels = np.r_[train_labels, np.ones(100)]

def get_grid(data, eps=0.01):
    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1
    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1
    return np.meshgrid(np.arange(x_min, x_max, eps),
                         np.arange(y_min, y_max, eps))

plt.rcParams['figure.figsize'] = (10,8)
plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, 
            cmap='autumn', edgecolors='black', linewidth=1.5)
plt.plot(range(-2,5), range(4,-3,-1));

from sklearn.tree import DecisionTreeClassifier

# параметр min_samples_leaf указывает, при каком минимальном количестве
# элементов в узле он будет дальше разделяться
clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=17)
# обучаем дерево
clf_tree.fit(train_data, train_labels)

# немного кода для отображения разделяющей поверхности
xx, yy = get_grid(train_data)
predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
plt.pcolormesh(xx, yy, predicted, cmap='autumn')
plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, 
            cmap='autumn', edgecolors='black', linewidth=1.5);

# используем .dot формат для визуализации дерева
from ipywidgets import Image
from io import StringIO
import pydotplus
from sklearn.tree import export_graphviz

dot_data = StringIO()
export_graphviz(clf_tree, feature_names=['x1', 'x2'], 
                out_file=dot_data, filled=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(value=graph.create_png())

# 2.0

n_train = 150        
n_test = 1000       
noise = 0.1
def f(x):
    x = x.ravel()

    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)

def generate(n_samples, noise):
    X = np.random.rand(n_samples) * 10 - 5
    X = np.sort(X).ravel()
    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) + \
        np.random.normal(0.0, noise, n_samples)
    X = X.reshape((n_samples, 1))

    return X, y

X_train, y_train = generate(n_samples=n_train, noise=noise)
X_test, y_test = generate(n_samples=n_test, noise=noise)

from sklearn.tree import DecisionTreeRegressor
     
reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)

reg_tree.fit(X_train, y_train)
reg_tree_pred = reg_tree.predict(X_test)

plt.figure(figsize=(10, 6))
plt.plot(X_test, f(X_test), "b")
plt.scatter(X_train, y_train, c="b", s=20)
plt.plot(X_test, reg_tree_pred, "g", lw=2)
plt.xlim([-5, 5])
plt.title("Decision tree regressor, MSE = %.2f" % (np.sum((y_test - reg_tree_pred) ** 2) / n_test))
plt.show()

# _________________Дерево практика

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
data = pd.read_csv('../../data/telecom_churn.csv')

data.drop(['State', 'Voice mail plan'], axis=1, inplace=True)
data['International plan'] = data['International plan'].map({'Yes': 1,
                                                             'No': 0})

y = data['Churn'].astype('int')
X = data.drop('Churn', axis=1)

from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np

# Выделим 70% выборки (X_train, y_train) под обучение и 30% будут отложенной выборкой (X_holdout, y_holdout). 
X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, 
                                                      test_size=0.3, 
                                                      random_state=17)

first_tree = DecisionTreeClassifier(random_state=17)
# np.mean(cross_val_score(first_tree, X_train, y_train, cv=5))

%%time
first_tree.fit(X_train, y_train)
tree_pred = first_tree.predict(X_holdout)
accuracy_score(y_holdout, tree_pred)

# метод ближайших соседей		
from sklearn.neighbors import KNeighborsClassifier
first_knn = KNeighborsClassifier()
# np.mean(cross_val_score(first_knn, X_train, y_train, cv=5))

%%time
first_knn.fit(X_train, y_train)
knn_pred = knn.predict(X_holdout)
accuracy_score(y_holdout, knn_pred)

# настраиваем max_depth для дерева

from sklearn.model_selection import GridSearchCV
tree_params = {'max_depth': np.arange(1, 11), 'max_features':[.5, .7, 1]}

tree_grid = GridSearchCV(first_tree, tree_params, cv=5, n_jobs=-1, verbose=True)

%%time
tree_grid.fit(X_train, y_train);
tree_grid.best_score_, tree_grid.best_params_

knn_params = {'n_neighbors': range(5, 30, 5) }#+ list(range(50, 100, 10))}
knn_grid = GridSearchCV(first_knn, knn_params, cv=5)

%%time
knn_grid.fit(X_train, y_train);
knn_grid.best_score_, knn_grid.best_params_

np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=1), X_train, y_train, cv=5))
knn = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)
accuracy_score(y_holdout, knn.predict(X_holdout))

tree_valid_pred = tree_grid.predict(X_valid)
from sklearn.metrics import accuracy_score
accuracy_score(y_valid, tree_valid_pred)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_valid, tree_valid_pred)
np.bincount(y_valid)

from sklearn.tree import export_graphviz
second_tree = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)
second_tree.score(X_valid, y_valid)

export_graphviz(second_tree, out_file='telecom_tree2.dot',
               feature_names=X.columns, filled=True,)


# _________________Дерево решение - задание

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix
%matplotlib inline
from matplotlib import pyplot as plt
import seaborn as sns


def write_to_submission_file(predicted_labels, out_file, train_num=891,
                    target='Survived', index_label="PassengerId"):
    # turn predictions into data frame and save as csv file
    predicted_df = pd.DataFrame(predicted_labels,
                                index = np.arange(train_num + 1,
                                                  train_num + 1 +
                                                  predicted_labels.shape[0]),
                                columns=[target])
    predicted_df.to_csv(out_file, index_label=index_label)

train_df = pd.read_csv("../../data/titanic_train.csv") 
test_df = pd.read_csv("../../data/titanic_test.csv") 

y = train_df['Survived']
test_df.describe(include='all')

train_df['Age'].fillna(train_df['Age'].median(), inplace=True)
test_df['Age'].fillna(train_df['Age'].median(), inplace=True)
train_df['Embarked'].fillna('S', inplace=True)
test_df['Fare'].fillna(train_df['Fare'].median(), inplace=True)

# One-Hot-Encoding
train_df = pd.concat([train_df, pd.get_dummies(train_df['Pclass'], 
                                               prefix="PClass"),
                      pd.get_dummies(train_df['Sex'], prefix="Sex"),
                      pd.get_dummies(train_df['SibSp'], prefix="SibSp"),
                      pd.get_dummies(train_df['Parch'], prefix="Parch"),
                     pd.get_dummies(train_df['Embarked'], prefix="Embarked")],
                     axis=1)
test_df = pd.concat([test_df, pd.get_dummies(test_df['Pclass'], 
                                             prefix="PClass"),
                      pd.get_dummies(test_df['Sex'], prefix="Sex"),
                      pd.get_dummies(test_df['SibSp'], prefix="SibSp"),
                      pd.get_dummies(test_df['Parch'], prefix="Parch"),
                    pd.get_dummies(test_df['Embarked'], prefix="Embarked")],
                     axis=1)    

train_df.drop(['Survived', 'Pclass', 'Name', 'Sex', 'SibSp', 
               'Parch', 'Ticket', 'Cabin', 'Embarked', 'PassengerId'], 
              axis=1, inplace=True)
test_df.drop(['Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked', 'PassengerId'], 
             axis=1, inplace=True)

train_df.shape, test_df.shape	
set(test_df.columns) - set(train_df.columns)
test_df.drop(['Parch_9'], axis=1, inplace=True)

# Обучите на имеющейся выборке дерево решений (DecisionTreeClassifier) максимальной глубины 2. 
# Используйте параметр random_state=17 для воспроизводимости результатов.
# Сделайте с помощью полученной модели прогноз для тестовой выборки
# Отобразите дерево с помощью export_graphviz и dot.

# Обучите на имеющейся выборке дерево решений (DecisionTreeClassifier). Также укажите random_state=17. 
# Максимальную глубину и минимальное число элементов в листе настройте на 5-кратной кросс-валидации с помощью GridSearchCV.

# tree params for grid search
tree_params = {'max_depth': list(range(1, 5)), 
               'min_samples_leaf': list(range(1, 5))}	
# Ваш код здесь
# Сделайте с помощью полученной модели прогноз для тестовой выборки.